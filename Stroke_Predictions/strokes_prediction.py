# -*- coding: utf-8 -*-
"""Strokes Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wuf4trFwhWh-tKHK1mgoFodVcS5T6IwY

# Data Understanding

Data yang digunakan adalah dataset “Stroke Predictions” yang diambil dari website Kaggle.com. Pada tahapan ini, kita akan melihat isi dari dataset yang kita pakai baik dari attribut maupun dimensi dari dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from imblearn.over_sampling import SMOTE

# load the dataset
url = 'https://raw.githubusercontent.com/Liszt87/Machine-Learning-Project/main/Stroke_Predictions/healthcare-dataset-stroke-data.csv'
strokes = pd.read_csv(url)
strokes

"""Seperti yang bisa kita lihat bahwa dataset ini memiliki 12 attribute dan dimensi (5110, 12), artinya dataset ini memiliki 5110 sample pada 12 attribut.

Kita akan melanjutkan untuk melihat informasi apa saja yang kita bisa dapatkan dari dataset tersebut dengan menggunakan fungsi info().
"""

#Melihat Informasi dataset
strokes.info()

"""Perhatikan bahwa dalam dataset ini memiliki 5 tipe data object, 4 tipe data int64, dan tipe data float64. Hampit dari setiap attribut memiliki 5110 sample, kecuali attribut bmi yang memiliki 4909 sample. Hal ini disebabkan terdapat nilai yang bernilai NaN sehingga jumlahnya tidak sama dengan yang lain. Masalah tersebut akan diselesaikan pada bagian EDA untuk menangani masalah Missing Value.

# Exploratory Data Analysis

Pada tahap ini, kita akan mengeksplor data dengan membuat visualnya. Dari pembuatan visual tersebut kita dapat mengetahui maksud dari data dan hubungan antar attribut serta kita dapat menanangani masalah pada data, seperti Missing Value issues.

## EDA - Deskripsi Variabel

Dataset ini memiliki 5110 sampel dan memiliki 12 kolom atribut, yaitu :
1.	id : Identifikasi individu yang unik.
2.	gender : Gender suatu individu, seperti “male, female, dan other”
3.	age : Umur Individu.
4.	hypertension : 1 jika individu mengalami hipertensi dan 0 jika tidak.
5.	heart_disease : 1 jika individu mengalami gangguan hati dan 0 jika tidak.
6.	ever_married : Suatu individu pernah manikah atau tidak.
7.	work_type : Jenis-jenis pekerjaan yang dilakukan suatu indvidu, seperti bekerja di pemerintahan, tidak bekerja, wirasuaha, dan private.
8.	Residence_type : Individu tinggal di daerah Urban atau Rural.
9.	avg_glucose_lvl : Tingkat rata-rata glukosa dalam darah.
10.	bmi : Index berat badan.
11.	smoking_status : Status individu, seperti perokok aktif, tidak pernah merokok, tidak diketahui, dan pernah merokok sebelumnya.
12.	stroke : 1 jika individu berpotensial terkena stroke dan 0 jika tidak.

Pada tahap ini, kita akan meilhat deskripsi variabel dari segi statistika pada fitur numerikal dengan menggunakan fungsi describe().
"""

#Melihat deskipsi variabel dari segi statistika
strokes.describe()

"""## Menangani Masalah Data

Seperti yang sudah dijelaskan sebelumnya, kita menemukan nilai NaN pada attribut bmi dan disnilah kita akan menangani masalah tersebut.

Memeriksa berapa banyak nilai NaN pada setiap attribute.
"""

#Melihat jumlah nilai NaN pada setiap attribute.
strokes.isnull().sum()

"""Dari hasil diatas, kita mendapatkan informasi bahwa terdapat 201 nilai NaN pada attribute bmi.

Untuk menangani masalah ini, maka kita akan mengisi nilai-nilai tersebut dengan pendekatan statistika. Disini saya menggunakan nilai mean untuk mengisi nilai yang kosong. Alasannya adalah karena attribut ini memiliki nilai numerik dan memiliki korelasi yang kecil jika kita pikir secara nyata dengan attribute lain.
"""

strokes['bmi'].fillna(strokes['bmi'].mean(),inplace=True)
strokes['bmi'].isnull().sum()

"""Setelah melakukan pengisian nilai dengan mean, kita mencoba untuk memeriksanya kembali apakah terdapat nilai NaN yang masih tersisa. Perhatikan bahwa hasil yang diperoleh setelah diperiksa adalah nol sehingga kita bisa simpulkan tidak ada nilai NaN lagi pada attribute bmi.

Kita akan periksa lagi apakah terdapat nilai yang berubah dalam segi statistik pada attribute bmi berubah dengan fungsi describe().
"""

strokes.describe()

"""Perhatikan bahwa terdapat nilai yang berubah setelah kita mengisikan nilai dengan mean, misal pada std yang semula adalah 7.854067 menjadi 7.698018.

## EDA - Univariate Analysis

Pada tahap ini kita akan mengeksplorasi informasi dari tiap attribute. Data akan divisualisasikan dengan bar chart untuk attribut kategorikal dan histogram untuk attribute numerikal. Tahap ini juga akan melihat banyak jumlah tiap kelas pada tiap attribute
"""

numerical_features = ['id', 'age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi', 'stroke']
categorical_features = ['gender', 'ever_married', 'work_type', 'Residence_type','smoking_status']

"""**Fitur Kategorikal**"""

#Fitur gender
feature = categorical_features[0]
count = strokes[feature].value_counts()
percent = 100*strokes[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

#Fitur ever_married
feature = categorical_features[1]
count = strokes[feature].value_counts()
percent = 100*strokes[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

#Fitur work_type
feature = categorical_features[2]
count = strokes[feature].value_counts()
percent = 100*strokes[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

#Fitur residence_type
feature = categorical_features[3]
count = strokes[feature].value_counts()
percent = 100*strokes[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

#Fitur smoking_status
feature = categorical_features[4]
count = strokes[feature].value_counts()
percent = 100*strokes[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Dari hasil diatas, kita dapatkan berapa banyak kelas tiap attribute dan berapa banyak jumlahnya tiap kelas pada tiap attribute. Pada attribute gender, memiliki 3 gender yang mana ini meruapakan hal yang aneh untuk kita. Masalah ini akan kita selesaikan diakhir tahap EDA.

**Fitur Numerikal**

Sama seperti sebelumnya, kita akan melihat jumlah kelas dari tiap attribute. Tetapi, karena ini data numerikal maka kita akan lebih memperhatikannya dengan pendekatan statistika. Perhatikan bahwa selain attribute avg_glucose_lvl dan bmi memiliki bentuk yang tidak beraturan. Sedangkan, 2 attribute yang disebutkan tadi memiliki makna statistik yang akan berpengaruh dalam transformasi data dan pembuatan model. Perhatikan juga bahwa terdapat ketidakseimbangan data pada attribute stroke dimana ini akan berakibat pada performa model dan hasil prediksi. Masalah tersebut akan diselesaikan pada tahap Transfromasi data
"""

strokes.hist(bins=50, figsize=(20,15))
plt.show()

"""## EDA - Multivariate Analysis

Pada tahapan ini, kita akan meilhat hubungan antara tiap fitur dengan label, yaitu stroke. Untuk fitur kategorikal kita akan menganalisis apakag tiap kelas mempengaruhi banyaknya orang yang terkena strokes. Hal yang sama juga akan kita lakukan pada fitur numerikal tetapi kita akan melihat korelasi tiap fitur dengan label agar kita bisa memilih fitur mana yang akan berguna untuk pembuatan model nantinya.

**Fitur Kategorikal**
"""

#Membuat bar chart tiap attribute kategorikal
cat_features = strokes.select_dtypes(include='object').columns.to_list()
     
for col in categorical_features:
  sns.catplot(x=col, y="stroke", kind="bar", dodge=False, height = 4, aspect = 3,  data=strokes, palette="Set3")
  plt.title("hubungan orang yang terkena stroke dengan - {}".format(col))

"""Perhatikan bahwa hubungan stroke antar kelas tiap attribute memiliki efek yang kecil, kecuali pada attribute work_tyoe dan residence type. Namun, karena memiliki pengaruh walaupun kecil maka kita akan memasukkannya sebagai fitur nantinya.

**Fitur Numerikal**
"""

sns.pairplot(strokes, diag_kind = 'kde')

"""Dengan melihat pairplot diatas, kita sulit menentukan fitur mana yang memiliki korelasi tinggi untuk dijadikan fitur. Oleh karena itu, kita akan melihatnya lebuh lanjut dengan menggunakan Correlation Matriks untuk melihat seberapa besar hubungan tiap fitur numerikal terhadap label."""

plt.figure(figsize=(10, 8))
correlation_matrix = strokes.corr().round(2)
     
# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Perhatikan bahwa hubungan antar fitur dan label bisa dikatakan kecil tetapi karena masalah tersebut maka kita akan semua fitur berdasarkan fakta real di dunia nyata sehingga attribute id akan di drop karena tidak memiliki efek apapun terhadap label.

# Data Preparation

Pada tahap ini, kita akan melakukan transformasi data agar data yang digunakan untuk model jaul lebih baik dan meningkatkan performa model. Transformasi data yang akan dilakukan antara lain adalah Menghilangkan Attribut Yang Tidak Dipakai & Menghilangkan Suatu Kelas Pada Suatu Attirbute ,Encoding terhadap fitur kategorikal, data balancing, splitting data, dan standarisasi.

**Menghilangkan Attribut Yang Tidak Dipakai & Menghilangkan Suatu Kelas Pada Suatu Attirbute**

Dari hasil EDA yang sudah kita lakukan, kita mendapatkan informasi yang kita butuhkan dan dapat memutuskan fitur yang mana yang akan dipakai. Namun, kita menemukan attribute yang dirasa kurang berguna untuk dijadikan fitur, yaitu attribute id. Alasannya adalah secara fakta id tidak ada hubungannya dengan kondisi individu terkena stroke atau tidak dan karena kita sudah menentukan korelasi tiap fitur numerik terhadap attribute stroke maka kita ketahui bahwa attribute tersebut memiliki tingkat korelasi yang rendah anatar attribute id dan stroke.

Kita juga menemukan kelas yang menurut kita adalah anomaly, misalkan pada attribute gender. Pada attribute tersebut, kita menemukan 3 gender tetapi kita ketahui bahwa di dunia ini terdapat 2 gender saja, yaitu laki-laki dan wanita.

Dari 2 permasalahan yang kita dapat, kita dapat melakukan dropping attribute untuk id dan menghilangkan sample yang memiliki kelas gender Other. Teknik yang digunakan untuk melakukan hal tersebut sangat sederhana. Kita hanya perlu melakukan drop pada attribute dengan menggunakan fungsi drop() dengam parameter ['id'], inplace = True, axis = 1. Dengan menggunakan fungsi dan parameter tersebut kita berhasil menghilangkan semua data pada attribute ID.

Untuk menghilangkan kelas other pada attribute gender maka kita perlu menghilangkannya dengan cara memanggil dataset strokes dengan parameter "strokes.gender != 'Other" sehingga kita dapat menghilangkan data dari baris yang mengandung gender other.
"""

#Menghilangkan attribute drop dari dataset
strokes.drop(['id'], inplace=True, axis=1)

#menghilangkan kelas other pada attribute gender
strokes = strokes[strokes.gender != 'Other'] 
strokes.head()

"""**Encoding Fitur Kateogrikal**

Kita ketahui bahwa dalam tiap fitur kategorikal memiliki kelas yang berbeda-beda jumlahnya sehingga untuk melakukan encoding terhadap fitur tersebut tidak boleh sembarangan. Maka dari itu kita akan menggunakan teknik yang berbeda terhadap untuk kasus kelas biner maupun lebih.

Untuk fitur yang memiliki kelas sebanyak dua kita bisa menggunakan teknik preprocessing pada library sklearn. Teknik ini akan membuat nilai kategorikal menjadi nilai biner.
"""

#Encoding data kategorikal untuk fitur yang memiliki 2 kelas
from sklearn import preprocessing
object_col = ["gender", "ever_married" ,"Residence_type"]
label_encoder = preprocessing.LabelEncoder()
for col in object_col:
    strokes[col]=  label_encoder.fit_transform(strokes[col])

strokes.head(2)

"""Untuk kasus fitur yang memiliki lebih dari 2 kelas maka kita bisa menggunakan fungsi get_dummies() dari library pandas. get_dummies() digunakan untuk manipulasi data. fungsi ini dapat mengubah data kategoris menjadi variabel dummy atau indikator. 


"""

#Memainpulasi data kategorikal menjadi variabel dummy dengan pandas
strokes = pd.get_dummies(strokes)
strokes.head(2)

"""**Membagi data Input dan Output**"""

X = strokes.drop(["stroke"],axis =1)
y = strokes["stroke"]

"""**Teknik SMOTE**

Kita ketahui bahwa dari tahapan EDA kita mendapatkan informasi bahwa terdapat ketidakseimbangan pada jumlah kelas pada attribute strokes. Hal ini akan mengakibatkan pada performa model yang tidak bisa memprediksi dengan baik. Oleh karena itu, kita akan menggunakan teknik SMOTE untuk mengatasi imbalancing data.
"""

#Penggunaan SMOTE pada imbalancing data stroke
sm = SMOTE(random_state=123)
X_sm , y_sm = sm.fit_resample(X,y)

print(f'''Shape of X before SMOTE:{X.shape}
Shape of X after SMOTE:{X_sm.shape}''',"\n\n")

print(f'''Target Class distributuion before SMOTE:\n{y.value_counts(normalize=True)}
Target Class distributuion after SMOTE :\n{y_sm.value_counts(normalize=True)}''')

"""Perhatikan bahwa presentase kelas pada attribute stroke sekarang sudah seimbang dikarenakan kita menggunakan SMOTE pada data kita untuk menyeimbangkan data.

**Splitting Data**

Sebelum data digunakan untuk model, kita harus membagi data menjadi data train dan data test. Pembagian data akan menggunakan train_test_split dari library sklearn.model_selection. Untuk pembagian porsi datanya kita akan membuat train data memiliki porsi sebesar 80% dan data test sebesar 20% serta random state yang dipakai adalah 777.
"""

#Splitting data 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size = .2, random_state = 777)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""**Standarisasi**

Algoritma machine learning memiliki performa lebih baik dan konvergen lebih cepat ketika dimodelkan pada data dengan skala relatif sama atau mendekati distribusi normal. Proses scaling dan standarisasi membantu untuk membuat fitur data menjadi bentuk yang lebih mudah diolah oleh algoritma. Kita akan menggunakan teknik StandarScaler dari library Scikitlearn untuk melakukan standarisasi
"""

#standarisasi data training dan test
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""# Model

Pada tahap ini kita akan membuat model untuk prediksi stroke. Model yang akan kita gunakan adalah KNN, Random Forest, Boosting Algorithm. Karena ini bukan masalah regresi maka kita akan menggunaan versi classifier untuk pembuatannya.
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'], 
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""**K-Nearest Neighbour**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score     

knn = KNeighborsClassifier(n_neighbors = 10)
knn.fit(X_train, y_train)
prediksi1 = knn.predict(X_test)
     
models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)
acc1 = accuracy_score(y_test, prediksi1)
print('Testing-set Accuracy score is:', acc1)
print('Training-set Accuracy score is:',accuracy_score(y_train,knn.predict(X_train)))

"""**Random Forest**"""

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestClassifier
     
# buat model prediksi
RF = RandomForestClassifier(random_state=777)
RF.fit(X_train, y_train)
prediksi2 = RF.predict(X_test)
     
models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)         
acc2 = accuracy_score(y_test, prediksi2)
print('Testing-set Accuracy score is:', acc2)
print('Training-set Accuracy score is:',accuracy_score(y_train,RF.predict(X_train)))

"""**Boosting Algorithm**"""

from sklearn.ensemble import GradientBoostingClassifier                 
     
boosting = GradientBoostingClassifier()                            
boosting.fit(X_train, y_train)
prediksi3 = boosting.predict(X_test)

models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)
acc3 = accuracy_score(y_test, prediksi3)
print('Testing-set Accuracy score is:', acc3)
print('Training-set Accuracy score is:',accuracy_score(y_train,boosting.predict(X_train)))

"""# Evaluasi Model

Pada tahap ini, kita akan melakukan pengevaluasian model dengan cara melihat metriks-metriks yang digunakan. Hal ini dikarenakan kita akan memilih performa model yang terbaik dengan melihat metriks yang dihasilkan. Metriks yang digunakan antara lain adalah MSE, akurasi, presisi, recall, dan F1
"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])
 
# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}
 
# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3 
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3
 
# Panggil mse
mse

#Plotting MSE untuk train dan test tiap model
fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Perhatikan bahwa MSE terkecil dimiliki oleh model dari Random Forest, artinya model tersebut memiliki potensi kecil untuk melakukan kesalahan prediksi. Selanjutnya kita akan mengevaluasi dari segi metriks akurasi, presisi, recall, dan F1."""

from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, classification_report
report1 = classification_report(y_test, prediksi1)
print(report1)

report2 = classification_report(y_test, prediksi2)
print(report2)

report3 = classification_report(y_test, prediksi3)
print(report3)

"""Perhatikan bahwa Random Forest tetap menjadi model yang terbaik diantara ketiga lainnya. Hal ini dikarenakan model tersebut dapat memprediksi kelas dari attribute stroke dengan akurasi, presisi, recall, dan F1 yang cukup tinggi. Kesimpulannya, model yang memiliki performa terbaik dimiliki oleh **Random Forest.**"""